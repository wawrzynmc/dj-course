# LLM Engine Selection
ENGINE=GEMINI  # or LLAMA_CPP

# Gemini Configuration
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL_NAME=gemini-2.5-flash

# OpenAI Configuration
OPENAI_API_KEY=your_api_key_here
GEMINI_MODEL_NAME=gpt-4.1-mini

# LLaMA Configuration (if ENGINE=LLAMA_CPP)
LLAMA_MODEL_PATH=/path/to/model.gguf
LLAMA_GPU_LAYERS=1
LLAMA_CONTEXT_SIZE=2048
LLAMA_FLASH_ATTENTION=false

# Sampling parameters (all clients)
# temperature: Controls randomness in generation (0.0 - 2.0). Lower = more deterministic, higher = more creative
# top_p: Nucleus sampling - cumulative probability threshold (0.0 - 1.0)
# top_k: Top-K sampling - number of highest probability tokens to consider (Note: OpenAI does not support this)
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
LLM_TOP_K=40
